{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b",
   "metadata": {
    "id": "469eccff-f2d9-47e6-b2e7-86b8401d8d9b"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612",
   "metadata": {
    "id": "57a81de7-1bfa-4e29-98d8-e98162bb7612"
   },
   "source": [
    "# Lab 8.5 - Prompting Large Language Models\n",
    "\n",
    "In this lab we will practise prompting with a few Large Language Models (LLMs) using Groq (not to be confused with Grok). Groq is a platform that provides access to their custom-built AI hardware via APIs, allowing users to run open-source models such as Llama.\n",
    "\n",
    "We shall see that while LLMs are powerful tools, how you ask a question or frame a task can dramatically influence the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a",
   "metadata": {
    "id": "7ca332d1-638f-44c8-9f20-eab2f62bec2a"
   },
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719",
   "metadata": {
    "id": "111d6485-4dd7-47d7-a0f4-eba2a6cb3719"
   },
   "source": [
    "Step 1: Sign up for a free Groq account at https://console.groq.com/home .\n",
    "\n",
    "Step 2: Create a new API key at https://console.groq.com/keys. Copy-paste it into an empty text file called 'groq_key.txt'.\n",
    "\n",
    "Running the next cell will then read in this key and assign it to the variable `groq_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56441b82-ddc5-46e9-9583-878e8a807b7f",
   "metadata": {
    "id": "56441b82-ddc5-46e9-9583-878e8a807b7f"
   },
   "outputs": [],
   "source": [
    "groqfilename = r'groq_key.txt' # this file contains a single line containing your Groq API key only\n",
    "try:\n",
    "    with open(groqfilename, 'r') as f:\n",
    "        groq_key = f.read().strip()\n",
    "except FileNotFoundError:\n",
    "    print(\"'%s' file not found\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88",
   "metadata": {
    "id": "606b6102-4a13-44bb-9324-d1f9aff8ac88"
   },
   "outputs": [],
   "source": [
    "#!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c",
   "metadata": {
    "id": "dd0b4dfc-47a9-46f9-ae1a-6241b07ba41c"
   },
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227",
   "metadata": {
    "id": "1eef37e2-24ba-4e6d-a8ce-87ade1ef1227"
   },
   "source": [
    "First create an instance of the Groq client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b",
   "metadata": {
    "id": "54fec468-0ae0-42e1-ab8c-087b97c9818b"
   },
   "outputs": [],
   "source": [
    "client = Groq(api_key=groq_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097cd77-7a8d-4502-ae6e-607782706b26",
   "metadata": {
    "id": "a097cd77-7a8d-4502-ae6e-607782706b26"
   },
   "source": [
    "The following code shows what models are currently accessible through Groq. `context_window` refers to the size of memory (in tokens) during a session and `max_completion_tokens` is the maximum number of tokens that are generated in an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4",
   "metadata": {
    "id": "33fc8058-c9c7-49b3-b294-cb4e12de20f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>object</th>\n",
       "      <th>created</th>\n",
       "      <th>owned_by</th>\n",
       "      <th>active</th>\n",
       "      <th>context_window</th>\n",
       "      <th>public_apps</th>\n",
       "      <th>max_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-86m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632165</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>meta-llama/llama-prompt-guard-2-22m</td>\n",
       "      <td>model</td>\n",
       "      <td>1748632101</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>512</td>\n",
       "      <td>None</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qwen/qwen3-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1748396646</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>40960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>meta-llama/llama-guard-4-12b</td>\n",
       "      <td>model</td>\n",
       "      <td>1746743847</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743877158</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meta-llama/llama-4-scout-17b-16e-instruct</td>\n",
       "      <td>model</td>\n",
       "      <td>1743874824</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>compound-beta-mini</td>\n",
       "      <td>model</td>\n",
       "      <td>1742953279</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qwen-qwq-32b</td>\n",
       "      <td>model</td>\n",
       "      <td>1741214760</td>\n",
       "      <td>Alibaba Cloud</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>compound-beta</td>\n",
       "      <td>model</td>\n",
       "      <td>1740880017</td>\n",
       "      <td>Groq</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>playai-tts-arabic</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682783</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>playai-tts</td>\n",
       "      <td>model</td>\n",
       "      <td>1740682771</td>\n",
       "      <td>PlayAI</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mistral-saba-24b</td>\n",
       "      <td>model</td>\n",
       "      <td>1739996492</td>\n",
       "      <td>Mistral AI</td>\n",
       "      <td>True</td>\n",
       "      <td>32768</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deepseek-r1-distill-llama-70b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737924940</td>\n",
       "      <td>DeepSeek / Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>allam-2-7b</td>\n",
       "      <td>model</td>\n",
       "      <td>1737672203</td>\n",
       "      <td>SDAIA</td>\n",
       "      <td>True</td>\n",
       "      <td>4096</td>\n",
       "      <td>None</td>\n",
       "      <td>4096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-3.3-70b-versatile</td>\n",
       "      <td>model</td>\n",
       "      <td>1733447754</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>whisper-large-v3-turbo</td>\n",
       "      <td>model</td>\n",
       "      <td>1728413088</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>whisper-large-v3</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>131072</td>\n",
       "      <td>None</td>\n",
       "      <td>131072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>llama3-70b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>distil-whisper-large-v3-en</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Hugging Face</td>\n",
       "      <td>True</td>\n",
       "      <td>448</td>\n",
       "      <td>None</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>llama3-8b-8192</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Meta</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>model</td>\n",
       "      <td>1693721698</td>\n",
       "      <td>Google</td>\n",
       "      <td>True</td>\n",
       "      <td>8192</td>\n",
       "      <td>None</td>\n",
       "      <td>8192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id object     created  \\\n",
       "13            meta-llama/llama-prompt-guard-2-86m  model  1748632165   \n",
       "12            meta-llama/llama-prompt-guard-2-22m  model  1748632101   \n",
       "7                                  qwen/qwen3-32b  model  1748396646   \n",
       "16                   meta-llama/llama-guard-4-12b  model  1746743847   \n",
       "9   meta-llama/llama-4-maverick-17b-128e-instruct  model  1743877158   \n",
       "1       meta-llama/llama-4-scout-17b-16e-instruct  model  1743874824   \n",
       "5                              compound-beta-mini  model  1742953279   \n",
       "2                                    qwen-qwq-32b  model  1741214760   \n",
       "17                                  compound-beta  model  1740880017   \n",
       "3                               playai-tts-arabic  model  1740682783   \n",
       "0                                      playai-tts  model  1740682771   \n",
       "21                               mistral-saba-24b  model  1739996492   \n",
       "8                   deepseek-r1-distill-llama-70b  model  1737924940   \n",
       "18                                     allam-2-7b  model  1737672203   \n",
       "4                         llama-3.3-70b-versatile  model  1733447754   \n",
       "19                         whisper-large-v3-turbo  model  1728413088   \n",
       "10                               whisper-large-v3  model  1693721698   \n",
       "14                           llama-3.1-8b-instant  model  1693721698   \n",
       "15                                llama3-70b-8192  model  1693721698   \n",
       "6                      distil-whisper-large-v3-en  model  1693721698   \n",
       "20                                 llama3-8b-8192  model  1693721698   \n",
       "11                                   gemma2-9b-it  model  1693721698   \n",
       "\n",
       "           owned_by  active  context_window public_apps  max_completion_tokens  \n",
       "13             Meta    True             512        None                    512  \n",
       "12             Meta    True             512        None                    512  \n",
       "7     Alibaba Cloud    True          131072        None                  40960  \n",
       "16             Meta    True          131072        None                   1024  \n",
       "9              Meta    True          131072        None                   8192  \n",
       "1              Meta    True          131072        None                   8192  \n",
       "5              Groq    True          131072        None                   8192  \n",
       "2     Alibaba Cloud    True          131072        None                 131072  \n",
       "17             Groq    True          131072        None                   8192  \n",
       "3            PlayAI    True            8192        None                   8192  \n",
       "0            PlayAI    True            8192        None                   8192  \n",
       "21       Mistral AI    True           32768        None                  32768  \n",
       "8   DeepSeek / Meta    True          131072        None                 131072  \n",
       "18            SDAIA    True            4096        None                   4096  \n",
       "4              Meta    True          131072        None                  32768  \n",
       "19           OpenAI    True             448        None                    448  \n",
       "10           OpenAI    True             448        None                    448  \n",
       "14             Meta    True          131072        None                 131072  \n",
       "15             Meta    True            8192        None                   8192  \n",
       "6      Hugging Face    True             448        None                    448  \n",
       "20             Meta    True            8192        None                   8192  \n",
       "11           Google    True            8192        None                   8192  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.groq.com/openai/v1/models\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "pd.DataFrame(response.json()['data']).sort_values(['created'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b438489-c102-434b-872d-e807169deef6",
   "metadata": {
    "id": "2b438489-c102-434b-872d-e807169deef6"
   },
   "source": [
    "The Groq client object enables interaction with the Groq REST API and a chat completion request is made via the client.chat.completions.create method.\n",
    "\n",
    "The most important arguments of the client.chat.completions.create method are the following:\n",
    "* messages: a list of messages (dictionary form) that make up the conversation to date\n",
    "* model: a string indicating which model to use (see [list of models](https://console.groq.com/docs/models))\n",
    "* max_completion_tokens: the maximum number of tokens that are generated in the chat completion\n",
    "* response_format: setting this to `{ \"type\": \"json_object\" }` enables JSON output\n",
    "* seed: sample deterministically as best as possible, though identical outputs each time are not guaranteed\n",
    "* temperature: between 0 and 2 where higher values like 0.8 make the output more random (creative) and values like 0.2 are more focused and deterministic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99216392-acc3-4a00-826b-c166a1e52534",
   "metadata": {
    "id": "99216392-acc3-4a00-826b-c166a1e52534"
   },
   "outputs": [],
   "source": [
    "# help(client.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7",
   "metadata": {
    "id": "4d17d3c7-d7b5-47ed-b514-40b0a3e577b7"
   },
   "source": [
    "As a first example, note how the messages input is given as a list of a dictionaries with `role` and `content` keys. This is in a ChatML format recognised by many LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81",
   "metadata": {
    "id": "9cfd33fb-15fc-409d-a5a9-e8770885df81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models are artificial intelligence (AI) systems that process and understand human language. They work by:\n",
      "\n",
      "1. **Training on vast amounts of text data**: The models are trained on massive datasets of text from various sources, such as books, articles, and websites.\n",
      "2. **Learning patterns and relationships**: Through complex algorithms, the models identify patterns, relationships, and structures within the text, including grammar, syntax, and semantics.\n",
      "3. **Generating text based on context**: When given a prompt or input, the model uses this learned knowledge to generate text that is likely to follow the context and pattern of the input.\n",
      "4. **Improving through iteration**: The models continuously learn and improve through feedback, fine-tuning, and additional training data.\n",
      "\n",
      "This enables large language models to perform tasks such as language translation, text summarization, and conversation generation, like our conversation right now.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {   \"role\": \"system\", # sets the persona of the model\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", # what the user wants the assistant to do\n",
    "            \"content\": \"Explain briefly how large language models work\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b60c2-4300-4338-b325-c3ea876c1afe",
   "metadata": {
    "id": "728b60c2-4300-4338-b325-c3ea876c1afe"
   },
   "source": [
    "The output is in Markdown format so the following line formats this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88845bb3-4a3e-403a-93ab-439c46aa1832",
   "metadata": {
    "id": "88845bb3-4a3e-403a-93ab-439c46aa1832"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Large language models are artificial intelligence (AI) systems that process and understand human language. They work by:\n",
       "\n",
       "1. **Training on vast amounts of text data**: The models are trained on massive datasets of text from various sources, such as books, articles, and websites.\n",
       "2. **Learning patterns and relationships**: Through complex algorithms, the models identify patterns, relationships, and structures within the text, including grammar, syntax, and semantics.\n",
       "3. **Generating text based on context**: When given a prompt or input, the model uses this learned knowledge to generate text that is likely to follow the context and pattern of the input.\n",
       "4. **Improving through iteration**: The models continuously learn and improve through feedback, fine-tuning, and additional training data.\n",
       "\n",
       "This enables large language models to perform tasks such as language translation, text summarization, and conversation generation, like our conversation right now."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c8c97b-1711-4355-b108-86bed769c109",
   "metadata": {
    "id": "e1c8c97b-1711-4355-b108-86bed769c109"
   },
   "source": [
    "## Text summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4",
   "metadata": {
    "id": "930cdd6b-8d44-4b3e-b161-0ebcd4600bc4"
   },
   "source": [
    "We start with a llama3-8b-8192, a model using just over 8 billion parameters with at most 8192 tokens produced as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2",
   "metadata": {
    "id": "b7b56002-d85b-44d6-a2d0-4b1513eeb4f2"
   },
   "source": [
    "Here is an article to be summarised from the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29",
   "metadata": {
    "id": "7c426f9f-c780-4ca6-b913-9b5c63b8bb29"
   },
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "SAN FRANCISCO, California (CNN) -- A magnitude 4.2 earthquake shook the San Francisco area Friday at 4:42 a.m. PT (7:42 a.m. ET), the U.S. Geological Survey reported. The quake left about 2,000 customers without power, said David Eisenhower, a spokesman for Pacific Gas and Light. Under the USGS classification, a magnitude 4.2 earthquake is considered \"light,\" which it says usually causes minimal damage. \"We had quite a spike in calls, mostly calls of inquiry, none of any injury, none of any damage that was reported,\" said Capt. Al Casciato of the San Francisco police. \"It was fairly mild.\" Watch police describe concerned calls immediately after the quake » . The quake was centered about two miles east-northeast of Oakland, at a depth of 3.6 miles, the USGS said. Oakland is just east of San Francisco, across San Francisco Bay. An Oakland police dispatcher told CNN the quake set off alarms at people's homes. The shaking lasted about 50 seconds, said CNN meteorologist Chad Myers. According to the USGS, magnitude 4.2 quakes are felt indoors and may break dishes and windows and overturn unstable objects. Pendulum clocks may stop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754257a-74ff-4747-a18e-7ebfbc82639c",
   "metadata": {
    "id": "9754257a-74ff-4747-a18e-7ebfbc82639c"
   },
   "source": [
    "**Exercise:**\n",
    "Summarise the story text using the following three prompts. Use the format given above but here there is no need to set the persona (i.e. only include one dictionary in the messages list when calling `client.chat.completions.create`.) Comment on any differences.\n",
    "\n",
    "1) \"Summarise the following article in 3 sentences.\"\n",
    "\n",
    "2) \"Give me a TL;DR of this text.\"\n",
    "\n",
    "3) \"What's the key takeaway here?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c372155-bdde-45a4-a184-fce4c4e8034c",
   "metadata": {
    "id": "9c372155-bdde-45a4-a184-fce4c4e8034c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake struck the San Francisco area, considered \"light\" by the USGS, which resulted in minimal damage and no reported injuries or significant damage, but still caused approximately 2,000 power outages and set off home alarms in Oakland.\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"Summarise the following article in 3 sentences. \", \"Give me a TL;DR of this text. \", \"What's the key takeaway here?\"]\n",
    "#content will be p + story for p in prompts\n",
    "\n",
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": p + story}]\n",
    ")\n",
    "\n",
    "print(p, '\\n', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df",
   "metadata": {
    "id": "3634dd68-a9c1-42a2-a1ef-eb1487cbf9df"
   },
   "source": [
    "Run the above code again below and note that the answers may differ. This is due to the probabilistic nature of LLM token generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14",
   "metadata": {
    "id": "f7dffda3-fb3b-4555-b6c6-9bbc33248c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarise the following article in 3 sentences.  \n",
      " A magnitude 4.2 earthquake struck the San Francisco area at 4:42 a.m. local time, causing about 2,000 customers to lose power. The US Geological Survey classified the quake as \"light\", meaning it is expected to cause minimal damage, and officials reported no injuries or significant damage. The quake was centered near Oakland, just east of San Francisco, and caused minor shaking that lasted around 50 seconds, triggering some minor damage and setting off alarms at homes.\n",
      "Give me a TL;DR of this text.  \n",
      " Here is a brief summary of the text:\n",
      "\n",
      "A magnitude 4.2 earthquake struck the San Francisco area at 4:42am, causing minimal damage and no injuries. Power was lost for about 2,000 customers, but calls to authorities were mostly inquiries rather than reports of damage. The quake was centered in Oakland, east of San Francisco, and lasted about 50 seconds.\n",
      "What's the key takeaway here? \n",
      " The key takeaway is that a magnitude 4.2 earthquake shook the San Francisco area, causing minimal damage and no reported injuries, but did leave about 2,000 customers without power.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "for p in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",\n",
    "                messages=[{\"role\": \"user\", \"content\": p + story}]\n",
    ")\n",
    "\n",
    "    print(p, '\\n', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a39af-3722-4b71-8d70-cbfa63593496",
   "metadata": {
    "id": "405a39af-3722-4b71-8d70-cbfa63593496"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f",
   "metadata": {
    "id": "97b4a80d-acb8-4099-b8e2-dba4a372369f"
   },
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf",
   "metadata": {
    "id": "2b0be4be-e054-4b9c-ad38-843c97d3afaf"
   },
   "source": [
    "**Exercise**: In this section adjust the `max_completion_tokens` and `temperature` settings below to obtain different responses. Show some examples with the prompt \"Continue the story: It was a great time to be alive\" with the model \"llama-3.1-8b-instant\".\n",
    "\n",
    "* max_completion_tokens - the maximum number of tokens to generate. Note that longer words are made of multiple tokens (set to 200 and 500)\n",
    "* temperature (positive number) - the higher the number the more random (creative) the output (set to 0.2, 0.8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50207704-94be-4309-8a82-dc1d22a063ee",
   "metadata": {
    "id": "50207704-94be-4309-8a82-dc1d22a063ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...at least, that's what everyone kept saying. The year was 2025, and the world was rapidly transforming into a futuristic utopia. Cities were sleek and high-tech, with towering skyscrapers that seemed to stretch up to the sky and cars that flew like birds. Virtual reality had become indistinguishable from reality itself, and people could live out entire alternate lives in the virtual world.\n",
      "\n",
      "You were a young adult, living in one of these futuristic cities. You had just graduated from a prestigious university, where you had studied cutting-edge technologies like quantum computing and artificial intelligence. Your classmates were the best of the best, and you had all shared a passion for innovation and progress.\n",
      "\n",
      "As you walked through the bustling streets, you couldn't help but feel a sense of excitement and optimism. Everywhere you looked, there were new technologies being demoed, new companies being launched, and new breakthroughs being announced.\n",
      "\n",
      "Your name was Eli, and you had always been fascinated by space exploration. You\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=200, do not have a temperature setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    max_completion_tokens=200,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63235fcc-3a61-4829-998b-37f2f104322b",
   "metadata": {
    "id": "63235fcc-3a61-4829-998b-37f2f104322b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"It was a great time to be alive,\" I said, smiling as I watched the sunset over the bustling city streets. The year was 1985, and the world was on the cusp of something new and exciting.\n",
      "\n",
      "I had just walked out of a record store, a vinyl copy of Prince's latest album clutched tightly in my hand. I couldn't wait to get home and listen to the smooth sounds of 'Purple Rain.' The music was pulsating with energy, a perfect reflection of the vibrant atmosphere that seemed to permeate every aspect of life.\n",
      "\n",
      "As I walked, I stumbled upon a group of friends who were gathered outside a nearby arcade. We exchanged laughs and jokes, reminiscing about the latest video game releases. I popped a quarter into a Pac-Man machine, marveling at the way the pixels danced across the screen as I devoured pellets and ghosts alike.\n",
      "\n",
      "Later that night, I met up with my friends at a local diner, where we indulged in late-night fries and burgers, discussing everything from the latest music trends to our favorite movies. We were the embodiment of a generation: carefree, optimistic, and eager to take on the world.\n",
      "\n",
      "As the night wore on, we decided to take a walk along the beach, the cool ocean breeze refreshing our faces as we gazed out at the endless expanse of water. It was moments like these that reminded me how magical it was to be alive during this particular time and place.\n",
      "\n",
      "But even with all the excitement and energy, I couldn't shake the feeling that something was on the horizon, waiting to change the course of our lives. The news was filled with stories of the Cold War, protests against social inequality, and the rise of personal computers. It was a time of great uncertainty, but also of unparalleled possibility.\n",
      "\n",
      "As I watched the stars twinkling above, I felt a sense of wonder and awe at the vast, uncharted territory that lay ahead of us. And I knew that no matter what the future held, it would be something truly remarkable, something that would forever be etched in our collective memory as a golden era of hope and promise.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set max_completion_tokens=500, do not have a temperature setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive\"}],\n",
    "    max_completion_tokens=500,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50",
   "metadata": {
    "id": "8ac91a1a-4f55-4531-bb5b-cdb436a87e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sun was shining brightly, casting a warm glow over the bustling streets of the city. People of all ages and backgrounds walked side by side, each with their own unique story to tell. The air was filled with the sweet scent of blooming flowers and the sound of laughter and music.\n",
      "\n",
      "As I walked through the crowded streets, I couldn't help but feel a sense of excitement and possibility. It was a great time to be alive, and I felt grateful to be a part of it. The world was changing at a rapid pace, and it seemed like anything was possible.\n",
      "\n",
      "I passed by a group of street performers, who were entertaining the crowd with their acrobatic feats and musical talents. I watched in awe as they spun and leaped through the air, their movements seemingly effortless. The crowd cheered and clapped along, and I couldn't help but join in.\n",
      "\n",
      "As I continued on my way, I noticed a small café tucked away on a quiet side street. The sign above the door read \"The Cozy Cup,\" and the aroma of freshly brewed coffee wafted out into the air. I pushed open the door and stepped inside, feeling a sense of warmth and comfort wash over me.\n",
      "\n",
      "The café was cozy and intimate, with plush armchairs and soft lighting. The walls were adorned with vintage posters and colorful artwork, and the atmosphere was relaxed and welcoming. I took a seat at the bar and ordered a cup of coffee, feeling grateful for the chance to take a break and enjoy the moment.\n",
      "\n",
      "As I sipped my coffee, I struck up a conversation with the barista, a friendly young woman named Sarah. We talked about everything from our favorite books and movies to our hopes and dreams for the future. I felt a sense of connection with her, and I knew that I had found a kindred spirit.\n",
      "\n",
      "As the afternoon wore on, I realized that I had been at the café for hours. The sun was beginning to set, casting a golden glow over the city. I finished my coffee and said goodbye to Sarah, feeling grateful for the chance to connect with her and experience the beauty of the world around me.\n",
      "\n",
      "It was indeed a great time to be alive, and I felt grateful to be a part of it. As I walked back out into the evening air, I felt a sense of hope and possibility that I had never felt before. Anything seemed possible, and I knew that I was ready to take on whatever the future held.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 0.2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 0.2,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d",
   "metadata": {
    "id": "27aef18a-eee4-4d30-9de9-f97a303ad78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As I stood on the rooftop, gazing out at the vibrant cityscape below, I couldn't help but feel a sense of excitement and wonder. It was the year 2050, and the world was a vastly different place from the one my grandparents had grown up in. Advances in technology had transformed every aspect of life, from the way people communicated to the way they traveled.\n",
      "\n",
      "The streets were bustling with flying cars and hyperloops, zipping along at incredible speeds and revolutionizing transportation as we knew it. People of all ages and backgrounds walked side by side, their eyes fixed on their augmented reality contact lenses, which provided a constant stream of information and entertainment.\n",
      "\n",
      "Despite the many changes that had taken place, there was a sense of continuity and familiarity that ran deep. People still laughed, loved, and lived life to the fullest. Music and art continued to thrive, and new forms of creative expression emerged with each passing day.\n",
      "\n",
      "As I took in the sights and sounds of the city, I spotted a group of friends gathered around a small, sleek device that seemed to be some sort of portable concert speaker. The thumping beat and energetic lyrics drew me in, and I found myself swaying to the music alongside the crowd.\n",
      "\n",
      "The performer on stage was a young woman named Luna, who had made a name for herself in the virtual reality scene with her mesmerizing digital performances. She was a pioneer in the field of mixed reality art, blending the boundaries between the physical and digital worlds in ways that no one had ever thought possible.\n",
      "\n",
      "As I watched Luna's energetic set, I felt a surge of excitement and inspiration. This was what it meant to be alive – to experience the world around us, to push the boundaries of what was thought possible, and to connect with others in ways that transcended time and space.\n",
      "\n",
      "And as I looked out at the sea of faces surrounding me, I knew that I was not alone. We were all in this together, part of a global community that was constantly evolving and growing. It was a great time to be alive, and I felt grateful to be a part of it.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 1, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 1,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf",
   "metadata": {
    "id": "ac38e059-e23a-44eb-92fc-3fec9c7bdcdf"
   },
   "source": [
    "Note what happens when the temperature is set too high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "931f8e00-928c-4218-8e74-8c56269fbfcb",
   "metadata": {
    "id": "931f8e00-928c-4218-8e74-8c56269fbfcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summer was hot and long, filling the city's streets with the tantalizing scent of BBQ cooking pits spewed forth sweet melodic laughter which resonanted deeply with those who heard it echoing throughout city squares filled with smiling faces who eagerly celebrated. This sense of jubilation seemed infectious to almost everyone - and for many reasons - but the atmosphere now teem with anticipation - it's almost impossible that something was brewing.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER (set temperature = 2, do not have a max_completion_tokens setting)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Continue the story: It was a great time to be alive.\"}],\n",
    "    temperature = 2,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c",
   "metadata": {
    "id": "f4e9bd9a-ad6b-409b-a751-023575868b6c"
   },
   "source": [
    "### Zero-shot and one-short prompting for question-answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6662f0a-c7e3-4235-972d-2771781a4e53",
   "metadata": {
    "id": "f6662f0a-c7e3-4235-972d-2771781a4e53"
   },
   "source": [
    "This section shows the impact of prompting on the response. Zero-shot prompting means we provide the prompt without any examples or additional context. Let us initially ask Mistral a question using no prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc",
   "metadata": {
    "id": "e2d48674-a9cc-4e5a-8035-afcc37a01dfc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chemical reactions involve the transformation of one or more substances into new substances. The interaction between two chemicals depends on several factors, including:\n",
       "\n",
       "1. **Chemical properties**: The type of chemical bonds present in each molecule, such as ionic, covalent, or hydrogen bonds.\n",
       "2. **Chemical reactivity**: The tendency of each molecule to react with other molecules, which is influenced by factors like electronegativity, polarizability, and reactivity.\n",
       "3. **Concentration**: The amount of each chemical present in the reaction mixture.\n",
       "4. **Temperature**: The energy available to facilitate the reaction.\n",
       "5. **Catalysts**: Substances that speed up the reaction without being consumed.\n",
       "\n",
       "Here's a general overview of how two chemicals might react:\n",
       "\n",
       "1. **Contact**: The two chemicals come into contact with each other, allowing them to interact.\n",
       "2. **Collision**: The molecules of the two chemicals collide with each other, which can lead to the formation of new chemical bonds.\n",
       "3. **Breaking and forming bonds**: The existing chemical bonds in the reactant molecules are broken, and new bonds are formed between the atoms of the reactants, resulting in the formation of new products.\n",
       "4. **Energy exchange**: Energy is transferred between the reactants and products, often in the form of heat or light.\n",
       "\n",
       "Some common types of chemical reactions include:\n",
       "\n",
       "1. **Synthesis**: Combination of two or more chemicals to form a new compound (e.g., 2H2 + O2 → 2H2O).\n",
       "2. **Decomposition**: Breakdown of a single compound into two or more simpler substances (e.g., 2H2O → 2H2 + O2).\n",
       "3. **Combustion**: Reaction of a substance with oxygen, releasing heat and light (e.g., CH4 + 2O2 → CO2 + 2H2O).\n",
       "4. **Neutralization**: Reaction of an acid with a base to form a salt and water (e.g., HCl + NaOH → NaCl + H2O).\n",
       "\n",
       "Keep in mind that this is a simplified explanation, and the specifics of chemical reactions can be complex and influenced by many factors. If you have a specific question about a particular reaction, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d",
   "metadata": {
    "id": "c7e7bac7-52f6-4779-a2a3-d45d29975f5d"
   },
   "source": [
    "**Exercise:** Ask the same question but modify the prompt to return the answer to the same question in a simpler form (still using the llama-3.1-8b-instant model). Experiment with different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6",
   "metadata": {
    "id": "92250b60-9d68-4906-80b8-db78fc9c2ae6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have two different kinds of playdough. One is blue and the other is red. They're like two different chemicals that don't usually mix together.\n",
       "\n",
       "When you put the blue playdough and the red playdough next to each other, they just sit there. But if you start to mix them together, something cool happens. The blue and the red start to combine and create a new color - like purple!\n",
       "\n",
       "This is kind of like what happens when two chemicals react. They're like the blue and red playdough, but instead of colors, they're tiny particles that don't usually mix together. When they mix, they can create something new and different.\n",
       "\n",
       "Imagine the blue playdough is like one chemical, let's call it \"A\". And the red playdough is like another chemical, let's call it \"B\". When they mix together, they create a new chemical, let's call it \"C\". This is like the purple color they make when they mix together.\n",
       "\n",
       "Chemical reactions happen all around us, and they're really cool. They help make things like bread rise, or flowers bloom. They're like tiny little magic tricks that happen when two chemicals mix together!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Answer the following question as though I am 10 years old. How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04676a11-57a7-4f9c-87e2-128559f0ffce",
   "metadata": {
    "id": "04676a11-57a7-4f9c-87e2-128559f0ffce"
   },
   "source": [
    "### One-shot prompting ###\n",
    "\n",
    "Next, note the dramatic change when we give the following template setting a new role and providing an English question followed by a French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473",
   "metadata": {
    "id": "fcb1c8fa-d3cb-400d-ae0f-3bc8f2d1d473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment deux composés chimiques réagissent-ils?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"system\",\n",
    "             \"content\": \"You translate English to French.\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"What time is it?\"},\n",
    "               {\"role\": \"assistant\",\n",
    "               \"content\": \"Quelle heure est-il?\"},\n",
    "              {\"role\": \"user\",\n",
    "               \"content\": \"How do two chemicals react?\"}],\n",
    "    temperature = 0.8,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269",
   "metadata": {
    "id": "fc1394e5-34b9-46b8-aea6-87a7862b7269"
   },
   "source": [
    "### Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050",
   "metadata": {
    "id": "d5e3b3d0-c365-4138-9be0-9d324bc34050"
   },
   "source": [
    "Recall that since the text generation process outputs one token at a time, their outputs often need adjusting. This is where examples can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b",
   "metadata": {
    "id": "68c6131b-3b30-45b0-8dab-0dc547033b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regrettably, I will be unable to attend the meeting.\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"I'm gonna head out now, see you later.\"\n",
    "response1 = \"I will be leaving now. See you later.\"\n",
    "\n",
    "prompt2 =  \"That movie was super cool!\"\n",
    "response2 = \"The movie was very impressive.\"\n",
    "\n",
    "prompt3 = \"Can't make it to the meeting, sorry.\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a professional editor. Rewrite casual sentences into a formal tone.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e9e18-cc27-453b-8844-3f683fb607f8",
   "metadata": {
    "id": "040e9e18-cc27-453b-8844-3f683fb607f8"
   },
   "source": [
    "The output can also be moulded to provide SQL output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed854534-30db-4745-b910-27d12a3ed47e",
   "metadata": {
    "id": "ed854534-30db-4745-b910-27d12a3ed47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM products WHERE quantity_in_stock = 0;\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"Show me all users who signed up in the last 30 days.\"\n",
    "response1 = \"SELECT * FROM users WHERE signup_date >= CURRENT_DATE - INTERVAL '30 days';\"\n",
    "\n",
    "prompt2 = \"What is the average order value?\"\n",
    "response2 =  \"SELECT AVG(order_total) FROM orders;\"\n",
    "\n",
    "prompt3 = \"List products that are out of stock.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that translates natural language to SQL.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt1},\n",
    "        {\"role\": \"assistant\", \"content\": response1},\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"assistant\", \"content\": response2},\n",
    "        {\"role\": \"user\", \"content\": prompt3},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7",
   "metadata": {
    "id": "45a1dda2-2913-4a59-bfc3-3f0a59f0dcc7"
   },
   "source": [
    "**Exercise**: Create a few examples to train the \"llama3-70b-8192\" LLM to take in user content in the form below and provide output as a pandas dataframe. Use the `exec` function to execute its output to display the answer of sample input as a data frame.\n",
    "\n",
    "Example:\n",
    "\n",
    "given the user content\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "| col1 | col2 | col3\n",
    "\n",
    "| 32 | 27 | 25\n",
    "\n",
    "| 64 | 23 | 14\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train the model to output\n",
    "\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b8ac08a-5759-41cd-a560-e77694573723",
   "metadata": {
    "id": "2b8ac08a-5759-41cd-a560-e77694573723"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colA</th>\n",
       "      <th>colB</th>\n",
       "      <th>colC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   colA  colB  colC\n",
       "0    23    12    54\n",
       "1     8    76    32\n",
       "2     7     5     3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANSWER\n",
    "user1 = \"\"\"col1 | col2 | col3\n",
    "32 | 27 | 25\n",
    "64 | 23 | 14\n",
    "\"\"\"\n",
    "\n",
    "output1 = \"\"\"\n",
    "df = pd.DataFrame({'col1': [32, 64], 'col2': [27, 23], 'col3': [25, 14]})\n",
    "\"\"\"\n",
    "\n",
    "user2 = \"\"\"col1 | col2\n",
    "23 | 12\n",
    "8 | 76\n",
    "7 | 5\n",
    "\"\"\"\n",
    "output2 = \"\"\"\n",
    "df = pd.DataFrame({'col1': [23, 8, 7], 'col2': [12, 76, 5]})\n",
    "\"\"\"\n",
    "user3 = \"\"\"colA | colB | colC\n",
    "23 | 12 | 54\n",
    "8 | 76 | 32\n",
    "7 | 5 | 3\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a data scientist who will receive data input as a string and provide output as a pandas dataframe called df. Use the examples to guide you\"},\n",
    "        {\"role\": \"user\", \"content\": user1},\n",
    "        {\"role\": \"assistant\", \"content\": output1},\n",
    "        {\"role\": \"user\", \"content\": user2},\n",
    "        {\"role\": \"user\", \"content\": output2},\n",
    "        {\"role\": \"user\", \"content\": user3}\n",
    "    ]\n",
    ")\n",
    "\n",
    "exec(response.choices[0].message.content.strip()) # string executed as Python code\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f62a88-c7b4-4431-acda-97e9a98981f4",
   "metadata": {
    "id": "29f62a88-c7b4-4431-acda-97e9a98981f4"
   },
   "source": [
    "Also show what happens when the question is asked in the absence of a system role and without few-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6",
   "metadata": {
    "id": "712221ea-14c0-4faa-a126-ca4f2f0538a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A beautiful table!\\n\\nIs there something specific you'd like to do with this table, or would you like me to suggest some operations or analyses?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user3}\n",
    "    ]\n",
    ")\n",
    "response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28660884-dfd4-4b96-a65d-d21ddbf99423",
   "metadata": {
    "id": "28660884-dfd4-4b96-a65d-d21ddbf99423"
   },
   "source": [
    "### Chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f85ef-afc0-4e31-9ad8-ae780c535837",
   "metadata": {
    "id": "724f85ef-afc0-4e31-9ad8-ae780c535837"
   },
   "source": [
    "The results of question-answering can also be improved by prompting the LLM to provide intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee04ad6-968b-406a-8119-981b01ef5f92",
   "metadata": {
    "id": "bee04ad6-968b-406a-8119-981b01ef5f92"
   },
   "source": [
    "**Exercise**: Using the following prompts, compare the answers of the \"llama3-8b-8192\" model (set seed=21). (If this model is no longer available choose a model with relatively few parameters.)\n",
    "\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb",
   "metadata": {
    "id": "bc8868f2-7c4f-4b99-bbd2-cc731cca74bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------zero-shot-prompt------\n",
      "There are 2 s's in the word 'success'.\n",
      "------chain-of-thought------\n",
      "To count the number of 's's in the word \"success\", I will go through each letter one by one:\n",
      "\n",
      "1. S\n",
      "   There is 1 's' so far.\n",
      "\n",
      "2. U\n",
      "   There's an 'U' and still 1 's' so far.\n",
      "\n",
      "3. C\n",
      "   There's a 'C', and still 1 's' so far.\n",
      "\n",
      "4. C\n",
      "   There's another 'C', and still 1 's' so far.\n",
      "\n",
      "5. E\n",
      "   There's an 'E', and still 1 's' so far.\n",
      "\n",
      "6. S\n",
      "   There's another 's', which means there are 2 's's so far.\n",
      "\n",
      "7. S\n",
      "   There's a third 's', which means there are 3 's's so far.\n",
      "\n",
      "So, the word \"success\" has 3 's's.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "zero_shot_prompt = \"How many s's are in the word 'success'?\"\n",
    "chain_of_thought_prompt = \"How many s's are in the word 'success'? Explain your answer step by step by going through each letter in turn.\"\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": zero_shot_prompt}],\n",
    "    seed = 21\n",
    ")\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": chain_of_thought_prompt}],\n",
    "    seed = 21\n",
    ")\n",
    "\n",
    "print('------zero-shot-prompt------')\n",
    "print(response1.choices[0].message.content)\n",
    "\n",
    "print('------chain-of-thought------')\n",
    "print(response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f",
   "metadata": {
    "id": "09499b6e-a4aa-439c-a785-3f0e06a3ba4f"
   },
   "source": [
    "## Comparison of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f",
   "metadata": {
    "id": "dd0e82d9-cefc-4af3-9d95-5e22db6cd56f"
   },
   "source": [
    "**Exercise**: Compare the performance of 2 LLMs by outputting the answers of the following questions into a dataframe.\n",
    "\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "\n",
    "Column headings:\n",
    "\n",
    "Model Name | Question | Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238",
   "metadata": {
    "id": "b5e0c587-9650-4082-baf4-d9cdd94bc238"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist break up with the statistician? \\n\\nBecause they had too many \"p\" values in their relationship! 😂  \\n\\n\\nLet me know if you'd like to hear another one!  🤖</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>Here's a way to calculate 22 * 13 mentally using breaking down numbers and known facts:\\n\\n**1. Break down 22:**  Think of 22 as (20 + 2)\\n\\n**2. Distribute:** Now you have (20 + 2) * 13.  We can distribute this:\\n   *  20 * 13 = 260 \\n   *  2 * 13 = 26\\n\\n**3. Add the results:** 260 + 26 = 286\\n\\n\\n**Therefore, 22 * 13 = 286**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma2-9b-it</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>Bartholomew \"Bart\" Bumble was a baby of considerable ambition. Donning a perpetually perplexed frown, he regarded the world from his vantage point on the living room floor. The floor, he decided, was a vast, uncharted territory, dotted with perilous mountains of plush toys and treacherous valleys of fluffy rugs.\\n\\nBart longed to explore this forbidden land, to reach the tantalizing banana slowly ripening on the counter, to wrestle the squeaky dragon that lay just out of arm's reach. But alas, he was bound by the limitations of his currently non-existent skills.\\n\\nThen came the day.\\n\\nHe had been practicing, pushing himself up on his chubby hands and wobbling about like a newborn giraffe. It was slow, it was frustrating, it was a whole lot of tumbling, but today felt different. Today was the day Bart deemed himself ready. \\n\\nHe took a deep breath, his tiny chest puffing out like a determined blimp. With a burst of effort, he pushed off the floor, legs flailing like a newborn gazelle on roller skates. He was moving! \\n\\nHis journey was a monument to perseverance. He'd scoot forward, legs locked in place, then lose momentum, tumbling onto his side with a resounding “thump.” He’d try again, arms flailing for balance, only to meet his fate head-on with a throw pillow mountain.\\n\\nHis parents watched with a mixture of amusement and pride, capturing the chaotic journey in a flurry of photos and “awws.” They cheered him on, offering gentle guidance and celebrating every inch he managed to conquer.\\n\\nThere were setbacks, of course. He would get stuck behind the sofa, mistaking it for a formidable fortress, or he'd find himself nose-to-nose with the family cat, Sparky, who eyed him with a mixture of curiosity and disdain.\\n\\nBut Bart was unyielding. Day after day, he practiced. He learned to anticipate the terrain, to navigate the treacherous pitfalls, to scoot with newfound precision.\\n\\nFinally, the day arrived.\\n\\nBartholomew Bumble, the baby with the perpetually perplexed frown, crawled. \\n\\nHe crawled straight to the banana. He grasped it with his pudgy fist and, with a triumphant gurgle, took a bite. It wasn't perfect, smeared all over his face, but it tasted sweeter than ever.\\n\\nHe was no longer a prisoner of his own limitations; he was an explorer, a conqueror, a master of the crawling cosmos. And from that day forward, his frown never quite returned. It was replaced by a mischievous grin, the grin of a baby who had discovered his newfound freedom, and the world was his to explore.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>Tell me a joke about data science.</td>\n",
       "      <td>Why did the data scientist quit his job?\\n\\nBecause he couldn't find any trends in his career path and his regression analysis was too depressing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>How can one calculate 22 * 13 mentally?</td>\n",
       "      <td>To calculate 22 * 13 mentally, you can use the following method:\\n\\n1. Break down 22 into 20 + 2.\\n2. Multiply 20 by 13: 20 * 13 = 260 (this can be done by multiplying 20 by 10 and then 20 by 3, adding the results together, 20*10=200 and 20*3=60, then adding 200 + 60 =260)\\n3. Multiply 2 by 13: 2 * 13 = 26\\n4. Add the results together: 260 + 26 = 286\\n\\nAlternatively, you can use a quick multiplication trick: \\n\\n22 * 13 can be broken down as 20 * 13 + 2 * 13. Since 20 * 10 = 200 and 200 * 10=200 and 20 * 3 = 60, we can use this and multiply 200 by the remaining power of ten that we have, in this case 1, and then add to 60*10 which is 600, multiply by 10 and add to result 600+200 = 800, to get our initial 260, which we know, we just add 60.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>Write a creative story about a baby learning to crawl.</td>\n",
       "      <td>**The Great Crawl Expedition**\\n\\nIn a cozy little nursery, surrounded by soft toys and colorful mobiles, a tiny human named Emily was getting ready to embark on a momentous journey. This was no ordinary day; it was the day she would learn to crawl.\\n\\nEmily was a curious three-month-old baby, with a mop of fluffy hair and bright inquiring eyes. From her earliest days, she had been practicing this new and mysterious skill, known as crawling. At first, it was just a series of wobbly movements, but with each passing day, she grew more confident.\\n\\nAs her mom, Sarah, placed Emily on the floor, she couldn't help but feel a mix of excitement and nervousness. What if Emily didn't get it? What if she got frustrated and gave up?\\n\\nEmily, however, had other plans. She gazed around the nursery, her eyes taking in the sights and sounds of her new surroundings. She spotted a soft ball on the floor, and with a determined look on her face, she lunged forward, attempting to grab it.\\n\\nSarah held her breath as Emily's tiny hands flailed wildly, but she quickly realized that she had to dig her fingers into the floor to propel herself forward. It was a slow, awkward motion, but it was a start.\\n\\nWith each passing moment, Emily grew more determined, her legs scrambling to get some traction as she pushed off the floor. Sarah cheered her on, offering words of encouragement: \"You can do it, Emily! Keep going!\"\\n\\nAs Emily crawled forward, she encountered all manner of obstacles, from a wobbly toy car to a loose thread on a blanket. But with each hurdle, she learned to adapt, using her developing muscles and coordination to navigate the terrain.\\n\\nSarah and her husband, Jack, watched in awe as Emily crawled across the nursery, her little face set in a determined expression. They exchanged a look of amazement – it was as if Emily was suddenly a different person, full of newfound energy and determination.\\n\\nAfter several minutes of crawling, Emily reached the far end of the nursery, where she encountered a large, fluffy pillow. With a triumphant cry, she planted her hands on the pillow and climbed to her feet, beaming with pride.\\n\\nSarah swept her up in a hug, exclaiming, \"Congratulations, Emily! You did it!\" Jack took a photo, capturing the moment forever in a heartwarming snapshot.\\n\\nFrom that day on, Emily was a new baby. She crawled, stood, and even took a few tentative steps, her confidence growing with each passing day. She was on a mission to explore the world, one crawling motion at a time.\\n\\nAs the family watched Emily play, they knew that this was just the beginning of an incredible journey – one that would take her to new heights and unexpected places.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model Name  \\\n",
       "0          gemma2-9b-it   \n",
       "1          gemma2-9b-it   \n",
       "2          gemma2-9b-it   \n",
       "3  llama-3.1-8b-instant   \n",
       "4  llama-3.1-8b-instant   \n",
       "5  llama-3.1-8b-instant   \n",
       "\n",
       "                                                 Question  \\\n",
       "0                      Tell me a joke about data science.   \n",
       "1                 How can one calculate 22 * 13 mentally?   \n",
       "2  Write a creative story about a baby learning to crawl.   \n",
       "3                      Tell me a joke about data science.   \n",
       "4                 How can one calculate 22 * 13 mentally?   \n",
       "5  Write a creative story about a baby learning to crawl.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Why did the data scientist break up with the statistician? \\n\\nBecause they had too many \"p\" values in their relationship! 😂  \\n\\n\\nLet me know if you'd like to hear another one!  🤖  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Here's a way to calculate 22 * 13 mentally using breaking down numbers and known facts:\\n\\n**1. Break down 22:**  Think of 22 as (20 + 2)\\n\\n**2. Distribute:** Now you have (20 + 2) * 13.  We can distribute this:\\n   *  20 * 13 = 260 \\n   *  2 * 13 = 26\\n\\n**3. Add the results:** 260 + 26 = 286\\n\\n\\n**Therefore, 22 * 13 = 286**  \n",
       "2                                                                                                                                                                       Bartholomew \"Bart\" Bumble was a baby of considerable ambition. Donning a perpetually perplexed frown, he regarded the world from his vantage point on the living room floor. The floor, he decided, was a vast, uncharted territory, dotted with perilous mountains of plush toys and treacherous valleys of fluffy rugs.\\n\\nBart longed to explore this forbidden land, to reach the tantalizing banana slowly ripening on the counter, to wrestle the squeaky dragon that lay just out of arm's reach. But alas, he was bound by the limitations of his currently non-existent skills.\\n\\nThen came the day.\\n\\nHe had been practicing, pushing himself up on his chubby hands and wobbling about like a newborn giraffe. It was slow, it was frustrating, it was a whole lot of tumbling, but today felt different. Today was the day Bart deemed himself ready. \\n\\nHe took a deep breath, his tiny chest puffing out like a determined blimp. With a burst of effort, he pushed off the floor, legs flailing like a newborn gazelle on roller skates. He was moving! \\n\\nHis journey was a monument to perseverance. He'd scoot forward, legs locked in place, then lose momentum, tumbling onto his side with a resounding “thump.” He’d try again, arms flailing for balance, only to meet his fate head-on with a throw pillow mountain.\\n\\nHis parents watched with a mixture of amusement and pride, capturing the chaotic journey in a flurry of photos and “awws.” They cheered him on, offering gentle guidance and celebrating every inch he managed to conquer.\\n\\nThere were setbacks, of course. He would get stuck behind the sofa, mistaking it for a formidable fortress, or he'd find himself nose-to-nose with the family cat, Sparky, who eyed him with a mixture of curiosity and disdain.\\n\\nBut Bart was unyielding. Day after day, he practiced. He learned to anticipate the terrain, to navigate the treacherous pitfalls, to scoot with newfound precision.\\n\\nFinally, the day arrived.\\n\\nBartholomew Bumble, the baby with the perpetually perplexed frown, crawled. \\n\\nHe crawled straight to the banana. He grasped it with his pudgy fist and, with a triumphant gurgle, took a bite. It wasn't perfect, smeared all over his face, but it tasted sweeter than ever.\\n\\nHe was no longer a prisoner of his own limitations; he was an explorer, a conqueror, a master of the crawling cosmos. And from that day forward, his frown never quite returned. It was replaced by a mischievous grin, the grin of a baby who had discovered his newfound freedom, and the world was his to explore.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Why did the data scientist quit his job?\\n\\nBecause he couldn't find any trends in his career path and his regression analysis was too depressing.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           To calculate 22 * 13 mentally, you can use the following method:\\n\\n1. Break down 22 into 20 + 2.\\n2. Multiply 20 by 13: 20 * 13 = 260 (this can be done by multiplying 20 by 10 and then 20 by 3, adding the results together, 20*10=200 and 20*3=60, then adding 200 + 60 =260)\\n3. Multiply 2 by 13: 2 * 13 = 26\\n4. Add the results together: 260 + 26 = 286\\n\\nAlternatively, you can use a quick multiplication trick: \\n\\n22 * 13 can be broken down as 20 * 13 + 2 * 13. Since 20 * 10 = 200 and 200 * 10=200 and 20 * 3 = 60, we can use this and multiply 200 by the remaining power of ten that we have, in this case 1, and then add to 60*10 which is 600, multiply by 10 and add to result 600+200 = 800, to get our initial 260, which we know, we just add 60.  \n",
       "5  **The Great Crawl Expedition**\\n\\nIn a cozy little nursery, surrounded by soft toys and colorful mobiles, a tiny human named Emily was getting ready to embark on a momentous journey. This was no ordinary day; it was the day she would learn to crawl.\\n\\nEmily was a curious three-month-old baby, with a mop of fluffy hair and bright inquiring eyes. From her earliest days, she had been practicing this new and mysterious skill, known as crawling. At first, it was just a series of wobbly movements, but with each passing day, she grew more confident.\\n\\nAs her mom, Sarah, placed Emily on the floor, she couldn't help but feel a mix of excitement and nervousness. What if Emily didn't get it? What if she got frustrated and gave up?\\n\\nEmily, however, had other plans. She gazed around the nursery, her eyes taking in the sights and sounds of her new surroundings. She spotted a soft ball on the floor, and with a determined look on her face, she lunged forward, attempting to grab it.\\n\\nSarah held her breath as Emily's tiny hands flailed wildly, but she quickly realized that she had to dig her fingers into the floor to propel herself forward. It was a slow, awkward motion, but it was a start.\\n\\nWith each passing moment, Emily grew more determined, her legs scrambling to get some traction as she pushed off the floor. Sarah cheered her on, offering words of encouragement: \"You can do it, Emily! Keep going!\"\\n\\nAs Emily crawled forward, she encountered all manner of obstacles, from a wobbly toy car to a loose thread on a blanket. But with each hurdle, she learned to adapt, using her developing muscles and coordination to navigate the terrain.\\n\\nSarah and her husband, Jack, watched in awe as Emily crawled across the nursery, her little face set in a determined expression. They exchanged a look of amazement – it was as if Emily was suddenly a different person, full of newfound energy and determination.\\n\\nAfter several minutes of crawling, Emily reached the far end of the nursery, where she encountered a large, fluffy pillow. With a triumphant cry, she planted her hands on the pillow and climbed to her feet, beaming with pride.\\n\\nSarah swept her up in a hug, exclaiming, \"Congratulations, Emily! You did it!\" Jack took a photo, capturing the moment forever in a heartwarming snapshot.\\n\\nFrom that day on, Emily was a new baby. She crawled, stood, and even took a few tentative steps, her confidence growing with each passing day. She was on a mission to explore the world, one crawling motion at a time.\\n\\nAs the family watched Emily play, they knew that this was just the beginning of an incredible journey – one that would take her to new heights and unexpected places.  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None) # allows wide dataframes to be viewed\n",
    "models = [\"gemma2-9b-it\", \"llama-3.1-8b-instant\"] #can edit this\n",
    "\n",
    "# ANSWER\n",
    "prompts = [\n",
    "    \"Tell me a joke about data science.\",\n",
    "    \"How can one calculate 22 * 13 mentally?\",\n",
    "    \"Write a creative story about a baby learning to crawl.\",\n",
    "]\n",
    "\n",
    "results = {'Model Name': [], 'Question': [], 'Answer': []}\n",
    "\n",
    "for model in models:\n",
    "    for prompt in prompts:\n",
    "        results['Model Name'].append(model)\n",
    "        results['Question'].append(prompt)\n",
    "        try:\n",
    "            output = client.chat.completions.create(model = model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "            results['Answer'].append(output.choices[0].message.content.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {model}: {e}\")\n",
    "            results['Answer'].append((prompt, \"ERROR\"))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5",
   "metadata": {
    "id": "8f3e2651-0bee-40bd-b5cc-3c2891b1adb5"
   },
   "source": [
    "### Bonus\n",
    "\n",
    "See if you can prompt an LLM to perform sentiment analysis (output 'Positive' or 'Negative' only) on a given piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a",
   "metadata": {
    "id": "d7169fd6-f1d1-4a5d-8fd9-21167a54416a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANSWER\n",
    "input1 = \"I absolutely loved the way the story unfolded.\"\n",
    "output1 = \"Positive\"\n",
    "\n",
    "input2 = \"The food was cold and completely flavorless.\"\n",
    "output2 = \"Negative\"\n",
    "\n",
    "input3 = \"She handled the situation with grace and professionalism.\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are amazing at sentiment analysis. Give the sentiment of the next sentence as the examples show.\"},\n",
    "        {\"role\": \"user\", \"content\": input1},\n",
    "        {\"role\": \"assistant\", \"content\": output1},\n",
    "        {\"role\": \"user\", \"content\": input2},\n",
    "        {\"role\": \"assistant\", \"content\": output2},\n",
    "        {\"role\": \"user\", \"content\": input3},\n",
    "    ]\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d",
   "metadata": {
    "id": "0387259e-0bf8-400e-97d9-d3f5688b2e4d"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279421d-cbca-4003-941e-c2d2bc2833a8",
   "metadata": {
    "id": "f279421d-cbca-4003-941e-c2d2bc2833a8"
   },
   "source": [
    "We worked with a few Large Language Models (LLMs) using Groq and experimented with prompting for summarisation, text completion and question-answering tasks.\n",
    "\n",
    "We also explored controlling the randomness (creativity) of output through the temperature setting and tried different types of prompting to achieve desired forms of output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f06d5e-7073-485b-b046-afe839ab844c",
   "metadata": {
    "id": "94f06d5e-7073-485b-b046-afe839ab844c"
   },
   "source": [
    "## References\n",
    "1. [Groq's prompting guide](https://console.groq.com/docs/prompting)\n",
    "2. [Groq's playground](https://console.groq.com/playground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61aab4a-1330-4762-9318-5635c3a97aa7",
   "metadata": {
    "id": "d61aab4a-1330-4762-9318-5635c3a97aa7"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2025 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
